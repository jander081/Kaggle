{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7522e20da3a378e4237eabee671b96ba3125bd3f"
   },
   "source": [
    "## Housing: XGB Meta-regressor with LGBM, SVR, RF --> score: 0.12090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ced85ee9e1f50f797ff3bb66df014051649b234"
   },
   "source": [
    "This notebook is mainly oriented towards ML workflow using sklearn pipelines.  In order to modify certain transformers for pipeline use, techniques from http://flennerhag.com/2017-01-08-Recursive-Override/ are used and can be viewed in the attached supporting_files. Models were modified slightly to include baysian optimizers with mixed results. You can find additional notebooks and documents at my github: https://github.com/jander081.\n",
    "\n",
    "This dataset contains an abundance of missing values. It is not enough, however, to simple choose an imputation technique and proceed. Missing values need to be explored in order to determine the best handling method. For example, many of the null values are related (i.e. no basement = no finished, basement base, etc). Additionally, it cannot be assumed that a feature with a high percentage of null values should be removed. The pool feature is an example of this as it's null values exceed 99.6%. However, since null values indicate the absence of a pool, thee presence of a pool can carry additional price information that should be included in the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b32407244e93550e8ed43e9d78ae7f0bb72baab"
   },
   "source": [
    "__Approach:__\n",
    "\n",
    "1. Missing values \n",
    "2. Engineer features\n",
    "3. Pipelines\n",
    "4. Stack models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdf291990467a93cc71a5946a86c88b77bc763d8"
   },
   "outputs": [],
   "source": [
    "# import module we'll need to import our custom module\n",
    "from shutil import copyfile\n",
    "\n",
    "# copy our file into the working directory (make sure it has .py suffix)\n",
    "copyfile(src = \"../input/supporting-files/housing_code.py\", dst = \"../working/housing_code.py\")\n",
    "copyfile(src = \"../input/supporting-files/housing_imports.py\", dst = \"../working/housing_imports.py\")\n",
    "copyfile(src = \"../input/supporting-files/housing_models.py\", dst = \"../working/housing_models.py\")\n",
    "copyfile(src = \"../input/github-code/pandas_feature_union.py\", dst = \"../working/pandas_feature_union.py\")\n",
    "copyfile(src = \"../input/github-code/__init__.py\", dst = \"../working/__init__.py\")\n",
    "\n",
    "# import all our functions\n",
    "from housing_imports import *\n",
    "from housing_code import *\n",
    "from housing_models import *\n",
    "from pandas_feature_union import *\n",
    "from __init__ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b5d61665c99cfa367def3efab89ad60b8d4301d"
   },
   "outputs": [],
   "source": [
    "# PRESERVE THE HOME IDS IN FROM THE TEST SET. MERGE THE TWO SETS FOR PROCESSING\n",
    "csv_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv').drop_duplicates()\n",
    "csv_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv').drop_duplicates()\n",
    "y = csv_train.iloc[:, -1]\n",
    "data = pd.concat([csv_train.iloc[:, 1:-1], csv_test.iloc[:, 1:]], axis=0)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "# print(data.shape)\n",
    "house_id = pd.DataFrame(csv_test.Id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9190d65085cbb849d6700085dc3655c3e209acb5"
   },
   "source": [
    "### EDA -> MISSING VALUES AND TARGET FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "687824181556f04db4b93b31b76157493d7e0ea4"
   },
   "outputs": [],
   "source": [
    "# COUPLE DIFFERENT TYPES OF NULL VALUES\n",
    "null_columns=data.columns[data.isnull().any()]\n",
    "data[null_columns].isnull().sum().head() # ABBREVIATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e53afdba1c081331d9320147969ec54a150fd464"
   },
   "outputs": [],
   "source": [
    "# NULL VALUES FOR GARAGE AND BASEMENT FOLLOW A TREND\n",
    "import missingno as msno\n",
    "cats = TypeSelector(np.object).fit_transform(data)\n",
    "nulls = cats[cats.columns[cats.isnull().any()]]\n",
    "msno.matrix(nulls);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c8b9f5798abb9090b753f1714cdec09be7faa69"
   },
   "outputs": [],
   "source": [
    "# CREATE ABSOLUTE TIME FEATURES -> EASY ANSWER. TACKING ON A STRING ENSURES\n",
    "# THAT THE NUMBER IS NOT ACCIDENTLY CONVERTED BACK TO A NUMERICAL LATER\n",
    "# ALSO KEEP NUMERICALS FOR BINNING\n",
    "# GarageYrBlt REMOVED\n",
    "\n",
    "years = ['YearBuilt', 'YearRemodAdd', 'YrSold']\n",
    "\n",
    "for colname in years:\n",
    "        data[colname + '_cat'] = data[colname].apply(lambda x: x if np.isnan(x) else 'year_' + str(int(x)))\n",
    "\n",
    "# PERMANENTLY TRANSFORM TO CAT - NOT NUMERICAL IN NATURE\n",
    "data['MSSubClass'] = data.MSSubClass.apply(lambda x: 'class ' + str(x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "556f667122e3a7871fa91e36a6cec2cf7bac4d83"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# NOT MUCH DIFFERENCE BETWEEN GarageYrBlt AND YearBuilt. BASICALLY, \n",
    "# THE VALUES DIFFER IF A GARAGE WAS ADDED. A NEW INDICATER FEATURE IS \n",
    "# MADE AND THE COLUMN IS DROPPEED\n",
    "\n",
    "for i in tqdm(range(0, data.shape[0])):\n",
    "    if np.isnan(data.GarageYrBlt[i]):\n",
    "        year = data.YearBuilt[i]\n",
    "        data.GarageYrBlt[i] = year\n",
    "        \n",
    "data.GarageYrBlt = data.GarageYrBlt.apply(lambda x: int(x))\n",
    "\n",
    "new_feat = []\n",
    "for i in range(0, data.shape[0]):\n",
    "    if data.GarageYrBlt[i] == data.YearBuilt[i]:\n",
    "        new_feat.append(0)\n",
    "    else:\n",
    "        new_feat.append(1)\n",
    "\n",
    "# CREATE AN INDICATOR DATAFRAME. THIS HELPS AVOID CONFUSION DURING FINAL\n",
    "# PREPROCESSING\n",
    "\n",
    "    \n",
    "data['Garage_added'] = new_feat\n",
    "data['Garage_added'] = data['Garage_added'].astype(\"bool\")\n",
    "data.drop(['GarageYrBlt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dbe6b6422c81e9d52a42877a2f6e67b33f8a250b"
   },
   "outputs": [],
   "source": [
    "# CREATE AN INDICTATOR FOR REMODEL\n",
    "new_feat = []\n",
    "for i in range(0, data.shape[0]):\n",
    "    if data.YearBuilt[i] == data.YearRemodAdd[i]:\n",
    "        new_feat.append(0)\n",
    "    else:\n",
    "        new_feat.append(1)\n",
    "        \n",
    "data['Remodeled'] = new_feat\n",
    "data['Remodeled'] = data['Remodeled'].astype(\"bool\")\n",
    "# CONVERT A FEW MORE BOOLS FOR FUN\n",
    "data['paved_street'] = data.Street.apply(lambda x: 1 if x == 'Pave' else 0).astype('bool')\n",
    "data['central_air'] = data.CentralAir.apply(lambda x: 1 if x == 'Y' else 0).astype('bool')\n",
    "data.drop(['Street', 'CentralAir'], axis=1, inplace=True)\n",
    "# WE'LL LEAVE YearRemodAdd FOR FEATURE SELECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e54fc8ab4aa7331fdf709e4be0d53e92b831efe"
   },
   "outputs": [],
   "source": [
    "# ADDING RELATIVE REFACTORED TIME FEATURES\n",
    "import datetime\n",
    "current = datetime.date.today()\n",
    "# print(current.year)\n",
    "data['sold_delta'] = current.year - data['YrSold'] \n",
    "data['built_delta'] = current.year - data['YearBuilt'] \n",
    "data['remodel_delta'] = current.year - data['YearRemodAdd'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e303d2703baaab7e1e2e62912601ee80dd8a8ad"
   },
   "outputs": [],
   "source": [
    "# GIVEN THE NUMBER OF NULLS, DISTRIBUTION OF VALUES, AND RELATIONSHIP\n",
    "# BETWEEN FEATURES, SOME NULLS WILL BE FILLED WITH MODE AND SOME WILL\n",
    "# BE CONVERTED TO A NEW CATEGORY, \"NONE\"\n",
    "\n",
    "none_list = ['Alley', \n",
    "       'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "       'BsmtFinType2', \n",
    "       'FireplaceQu', 'GarageType', 'GarageFinish',\n",
    "       'GarageQual', 'GarageCond', 'Fence', 'MiscFeature', 'PoolQC']\n",
    "\n",
    "for colname in none_list:\n",
    "    data[colname].fillna('None', inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "339e379105d1522795848e7413a952d92515db5d"
   },
   "outputs": [],
   "source": [
    "# LOOKS READY FOR THE PIPELINES\n",
    "null_columns=data.columns[data.isnull().any()]\n",
    "print(data[null_columns].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ee263f78232aac4b11cc035479f04ad0c7362b9"
   },
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2825d55038ec90ab9c4474d37856ec8bc3be45d2"
   },
   "source": [
    "The remain preprocessing has been abstracted using sklearn pipelines and a mix of transformers. Some of the transformers are lesser known (i.e. SoftImpute), some are wrapped standard transformers (i.e. KBins, StandardScaler), and some are custom (i.e. RegImpute, FreqFeatures). All of the wrapped classes can be viewed in attachments. The main purpose for the wrappers was to allow for pipeline use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f997199853c8832895cf859edffb67dd3cd0ffa3"
   },
   "outputs": [],
   "source": [
    "# I'VE ABSTRACTED AWAY MOST OF THE PREPROCESSING AND BASIC ENGINEERING. THE\n",
    "# TRANSFORMERS ARE VIEWABLE IN ATTACHMENTS. SOME OF THE TRANSFORMERS ARE SIMPLY\n",
    "# WRAPPERS THAT ALLOW THE TRANSFORMERS TO FUNCTION IN AN SKLEARN PIPELINE.\n",
    "\n",
    "transformer_list=[\n",
    "        (\"binned_features\", make_pipeline(\n",
    "                        TypeSelector(np.number),\n",
    "                        StandardScalerDf(),\n",
    "                        SoftImputeDf(),\n",
    "                        SelectFeatures(),\n",
    "                        KBins()\n",
    "        )),\n",
    "        (\"numeric_features\", make_pipeline(\n",
    "                            TypeSelector(np.number),\n",
    "                            StandardScalerDf(),\n",
    "                            SoftImputeDf()\n",
    "        )),\n",
    "        (\"categorical_features\", make_pipeline(\n",
    "                             TypeSelector(np.object),\n",
    "                             RegImpute() \n",
    "        )),\n",
    "        (\"frequency_features\", make_pipeline(\n",
    "                         TypeSelector(np.object),\n",
    "                         RegImpute(),\n",
    "                         SelectFeatures(val_count=15, categorical=True),\n",
    "                         FreqFeatures()\n",
    "        )),\n",
    "        (\"boolean_features\", make_pipeline(\n",
    "                         TypeSelector(np.bool_),\n",
    "                         RegImpute(regex=False) \n",
    "        ))  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7febbda7572334e76cc837c88e4ea712fa6a9391"
   },
   "source": [
    "__PandasFeatureUnion__ simply corrects for the numpy array output of sklearn FeatureUnion. It can be downloaded from Github at: https://github.com/marrrcin/pandas-feature-union\n",
    "\n",
    "__QuickPipeline__ is an awesome preprocessing pipeline that can be found at: https://github.com/Mottl/quickpipeline.\n",
    "\n",
    "QuickPipeline is slightly modified below to work in an sklearn pipeline. Basically, it inherits TransformerMixin and is given fit & transform functions rather than only fit_transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a17757ecdd35cb8c9f2a154369ce894710d128ac"
   },
   "outputs": [],
   "source": [
    "preprocess_pipeline = make_pipeline(\n",
    "    PandasFeatureUnion(transformer_list),\n",
    "    QuickPipeline_mod()  )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7854c06ee3a5a745fffe9ba7b43a1fae23799ac"
   },
   "outputs": [],
   "source": [
    "X = preprocess_pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8269b70f9418633cee161ba1485e3ebf3b06b783"
   },
   "outputs": [],
   "source": [
    "# I TRY TO KEEP THINGS IN PANDAS MOST OF THE TIME. THIS COMES IN HANDY WHEN \n",
    "# ANALYZING FEATURES LATER ON. \n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e20c4ccdf108a952cd6f19b27c02e547e72aa73"
   },
   "outputs": [],
   "source": [
    "# TAKING THE LOG OF THE TARGET CORRECTS FOR A RIGHT SKEW. HOWEVER, THE LOGNORMAL\n",
    "# CREATES A SLIGHT LEFT SKEW\n",
    "# NORMALIZATION IS FOR COMPARISON PURPOSES ONLY\n",
    "# SEPARATED FOR COMPARISON ONLY\n",
    "\n",
    "y_norm = y.apply(lambda x: (x - y.mean()) / y.std())\n",
    "y_box, lambda_ = boxcox(y) # need the lambda to eventually reverse the transformation\n",
    "y_box_norm = pd.DataFrame(y_box).apply(lambda x: (x - y_box.mean()) / y_box.std())\n",
    "sns.kdeplot(y_norm, label='normal')\n",
    "sns.kdeplot(np.log(y_norm) + 4, label='lognormal')\n",
    "sns.kdeplot(np.ravel(y_box_norm) - 4, label='box-cox')\n",
    "plt.legend();\n",
    "\n",
    "# THE LEAST SKEWED IS THE BOX COX TRANSFORMATION\n",
    "y, lambda_ = boxcox(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bad8552890664d3465400d83060f7bd01ab5a37a"
   },
   "outputs": [],
   "source": [
    "# MAKE SURE THE BOX COX IS REVERSIBLE. THE POWER TRANSFORM AVAILABLE \n",
    "# THROUGH SKLEARN DOES NOT SEEM TO PROVIDE A LAMBDA AND IS NOT EASILY REVERSIBLE\n",
    "\n",
    "def invboxcox(y,ld):\n",
    "    if ld == 0:\n",
    "        return(np.exp(y))\n",
    "    else:\n",
    "        return(np.exp(np.log(ld*y+1)/ld))\n",
    "\n",
    "#  LITTLE TEST TO MAKE SURE\n",
    "# test = csv_train.iloc[:, -1][:100]; print(test[:3])\n",
    "# y_box_test, lambda_test = boxcox(test); print(pd.Series(y_box_test)[:3])\n",
    "# # Add 1 to be able to transform 0 values\n",
    "# test_rev = invboxcox(y_box_test, lambda_test);print(pd.Series(test_rev).apply(lambda x: np.int64(x))[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48b1193013c02dba321cc582d953a8e6c974ff39"
   },
   "outputs": [],
   "source": [
    "X_test = X.iloc[1460:, :]\n",
    "X_ = X.iloc[:1460, :]\n",
    "print(X_.shape);print(X_test.shape);print(y.shape)\n",
    "X_train = X_\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2da987722e67a4121ca85f705acdfba9cc7d8968"
   },
   "source": [
    "### Stacked Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e98eef42c541535556ac71d9b252b2a715198ddf"
   },
   "source": [
    "You'll notice the __Bayes__ wrapper around each model. If you look at models.py, you'll see that a bayes optimizer is embedded in each model. Intervals are defined using dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6eb805155f2f79f015a3782031dc46c8dbc56011"
   },
   "outputs": [],
   "source": [
    "xgb_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14a8fef73417e90dfc16db2ddf2e99223d631bc7"
   },
   "source": [
    "While bayesian hyperparameter optimization is an exciting field, it is a bit harder to implement than one may initially assume. Simply defining large intervals for the optimizer to use will generally result in poorer performance. I explored this in detail in my master's thesis. Basically, one can spend a lifetime trying to understand a given hyperparameter's affect on model behavior, only to see the model completely misbehave or respond differently given another hyperparameter's interval. The best method I've found (so far) is to obtain a decent understanding of the given hyperparameter and default setting. Using a smaller random subset of data, begin testing different intervals using cross validation scores. I'll also search online for hyperparameters used for similarly sized data and test intervals based off values used by other data scientists. \n",
    "\n",
    "I'm also exploring combining a grid search method (for intervals) with bayesian techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33a881fba3ef3334c39c98cbffa734006a2f3116"
   },
   "outputs": [],
   "source": [
    "# REGRESSORS\n",
    "lgb = BayesLGBMRegressor()\n",
    "# LGBM WOULD NOT ACCEPT A DICT AS A HYPERPARAMETER - I'D NEED TO EXPLORE THIS MORE, USE **kwargs\n",
    "svr = BayesSVR(intervals=svr_params)\n",
    "rf = BayesRandomForest(intervals=rf_params)\n",
    "regressors = [lgb, svr, rf]\n",
    "\n",
    "# META-REGRESSOR\n",
    "meta = BayesXGBRegressor(intervals=xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "25a92bbeedee6eadd921232971926d943bbae8a2"
   },
   "outputs": [],
   "source": [
    "# RELATIVELY STRAIGHTFORWARD. EASY TO DISSECT AND UNDERSTAND THE RELATIONSHIP\n",
    "# BETWEEN THE META-MODEL AND BASE MODELS.\n",
    "\n",
    "ensemble = StackingCVRegressorAveraged(regressors=regressors, \n",
    "                                       meta_regressor=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9a846a12823f0f3f5b19a00c2b1003652c4abdac"
   },
   "outputs": [],
   "source": [
    "ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cf3f15fecfa8fb1b91050712aa46b3a47f817ef"
   },
   "outputs": [],
   "source": [
    "y_pred = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78d4ad0125cabf384ed140330d0dfafb8756ea65"
   },
   "outputs": [],
   "source": [
    "# REVERSE BOX COX TRANSFORMATION\n",
    "labels = pd.DataFrame(invboxcox(y_pred, lambda_)).apply(lambda x: np.float64(round(x, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5679cd8a5e9ddaddd7d6d30204f724893ebc423c"
   },
   "outputs": [],
   "source": [
    "submit = pd.concat([house_id, labels], axis=1)\n",
    "\n",
    "submit.set_index('Id', inplace=True)\n",
    "\n",
    "submit.rename(columns={0: 'SalePrice'}, inplace=True)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b8d2b968fcbc986a2db657558656dc2baebb83e"
   },
   "outputs": [],
   "source": [
    "# submit.to_csv('preds/predictions5.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e5c8124dd1e6ccba74799f07e742ace2829c1df"
   },
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ef11ec7b9b364a41db6153e5eb119200d329cd0"
   },
   "source": [
    " __Notes__: While looking over the data description, it seemed like a lot of the categorical features could be converted to ordinal for additional \n",
    "features. I've included the transformations below. The ensemble performance unfortunately did not improve. However, that could easily be the result of the optimier intervals or ensemble method (or a host of other factors). It seems that adding an ordinal aspect to a categoical feature (when appropriate) should capture more information for model use. I'd love to explore feature engineering with this dataset further, but unfortunately I need to move on. Perhaps someone can build upon the code provided below. Anyways, __happy kaggling!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aefa809371dbd62622772ffc73a1402cddf71788"
   },
   "outputs": [],
   "source": [
    "# SOME OF THE CATEGORICALS CAN ALSO BE REPRESENTED AS ORDINALS\n",
    "# data['BsmtQual'].value_counts(dropna=False)\n",
    "# cat_to_ordinal_1 = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, \n",
    "#                     'Ex': 5}\n",
    "\n",
    "# cat_to_ordinal_2 = {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, \n",
    "#                     'ALQ': 5, 'GLQ': 6}\n",
    "\n",
    "# Functionality = {'Sal Salvage': 0, 'Sev Severely': 1, 'Maj2': 2, 'Maj1': 3,\n",
    "#                 'Mod': 4, 'Min2': 5, 'Min1': 6, 'Typ': 7}\n",
    "\n",
    "# exposure = {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7832ed2dac95b7f096d3b1420af2e50de2b9b355"
   },
   "outputs": [],
   "source": [
    "# cat_to_num_1 = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC',\n",
    "#                'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond',\n",
    "#                'PoolQC']\n",
    "\n",
    "# cat_to_num_2 = ['BsmtFinType1', 'BsmtFinType2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e53ba0bc6c323fb84a637b56faf9959cb050f04"
   },
   "outputs": [],
   "source": [
    "# for feat in cat_to_num_1:\n",
    "#     data[feat].fillna(data[feat].mode()[0], inplace=True)\n",
    "#     data[feat + '_num'] = data[feat].map(cat_to_ordinal_1)\n",
    "\n",
    "# for feat in cat_to_num_2:\n",
    "#     data[feat].fillna(data[feat].mode()[0], inplace=True)\n",
    "#     data[feat + '_num'] = data[feat].map(cat_to_ordinal_2)\n",
    "\n",
    "# data['bsmt_exp_num'] =  data['BsmtExposure'].map(exposure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "854dcd8ef0ffa41e1b98a2cd69c68eb7d55b2acd"
   },
   "source": [
    "__References:__\n",
    "\n",
    "https://github.com/travisbrady/py-soft-impute\n",
    "\n",
    "http://arxiv.org/abs/1410.2596 \n",
    "Awesome paper. I break soft thresholding down fairly well in my thesis available at my github (use TOC - it's long). I'll also try to include a notebook that really breaks down solft thresholding using numpy.linalg. \n",
    "\n",
    "__other kagglers:__ \n",
    "\n",
    "https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "\n",
    "https://www.kaggle.com/neviadomski/how-to-get-to-top-25-with-simple-model-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d73c8c88c3683cacb087c42e2d1312568ba7f3b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
