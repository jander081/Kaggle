{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"## TF using Google Datalab"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"For those of you who don't already know, Google Cloud Services offers a cloud tool built on Jupyter.  You can learn more about it at https://cloud.google.com/datalab/. Using the service requires a credit card and some familiarity with google cloud. This notebook contains some useful code for importing and exporting data (csv) files.  You can start learning about Datalab at https://cloud.google.com/datalab/docs/quickstart. \n\nThe best score I achieved with this model was __.98942__. I'm sure I can get it up there a bit more with endless tweaking and training, but the score is decent and definitely served as a tensorflow refresher for me. Most of the notebook contents were borrowed from other tutorials and kernels (not trying to reinvent the wheel here). The main difference between this and other kernels is that a (cloud) Nvidia Tesla K80 GPU was used through the Datalab platform.  This model is a bit rough on CPUs, so I'll run the code in the following notebook using truncated datasets. \n\nYou may need to request a quota increase in order to access GPUs. In which case, Google support is pretty quick to respond.\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np, tensorflow as tf, sys\n\nimport matplotlib.pyplot as plt, matplotlib.cm as cm\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0338fc39ec066baadb74402ebb48d90484a45d4a"},"cell_type":"markdown","source":"> # Datalab**\n\nThe following code must be run in a cloud datalab notebook."},{"metadata":{"trusted":true,"_uuid":"7d6726c0383843fe4a0fb87a50aab2335187aa7b","_kg_hide-output":true},"cell_type":"code","source":"import google.datalab.storage as storage\nfrom io import BytesIO","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2b93548158b0aad3cdf98e31a644960ba030e90","_kg_hide-output":true},"cell_type":"code","source":"# ONCE YOUR DATA IS UPLOADED INTO A BUCKET, THE PATH CAN BE FOUND \n# UNDER THE OVERVIEW TAB\n%%gcs list --object gs://pathtobucket\n# this will return a list of a given bucket's contents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27b0db2fd39205260a2c8cfa2abab07281027353","_kg_hide-output":true},"cell_type":"code","source":"# MUST BE RUN IN A SEPARATE CHUNK\n%gcs read --object \"gs://digits_081/test.csv\" --variable data_test\n# the variable assignment is whatever you like","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad8eb868f507c8750ad6e539d2b3d1eb493e23df","_kg_hide-output":true},"cell_type":"code","source":"# CONGRATS! YOU NOW HAVE YOUR DATAFRAME\nX_test = pd.read_csv(BytesIO(data_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2f2f81380964302bd57c7209f95691935583b93"},"cell_type":"markdown","source":"## Non-Datalab\n\nThe code can be run locally."},{"metadata":{"trusted":true,"_uuid":"371cff756ab5bf70c18d725e1585cb173927bf3d"},"cell_type":"code","source":"# TRUNCATED THE DATASETS IN ORDER TO RUN ON CPU\nX = pd.read_csv('../input/train.csv').iloc[:5000, :] \nX_test = pd.read_csv('../input/test.csv').iloc[:500, :] \n\nlabels = X.iloc[:, 0]\nX.drop(['label'], axis=1, inplace=True)\n\nprint(X.shape); print(X_test.shape); print(labels.shape); X.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"598a328a8ff4f5bd851a944e13a2839aa16767fa"},"cell_type":"code","source":"image_size = X.shape[1]\nprint(\"Number of pixels per image: {} ranging {} to {}\".format(image_size, X.values.min(), X.values.max()))\n\nimage_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\nprint(\"Image size: {}x{}\".format(image_width, image_height))\n\nnum_colors = 1 # black and white","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"defdaddf567b67634d8f7d246fad8b5f7d43147e"},"cell_type":"code","source":"for i in range(0, 9):\n    image = X.iloc[i, :].values\n    plt.subplot(3, 3, i+1)\n    plt.imshow(image.reshape(image_width, image_height), cmap=cm.binary)\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59d2cdd6f8326eddf01d52e5ffe2c59e2c37cc63"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, labels,\n                                                      test_size=0.1, \n                                                      random_state=81)\ndframes = [X_train, X_valid, y_train, y_valid]\n\nfor frame in dframes:\n    frame.reset_index(drop=True, inplace=True)\n    print(frame.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da5490f9e8c92f89d65dab5853ca99cb6a52d892"},"cell_type":"code","source":"# CONVERT TO ARRAY AND RESHAPE\nX_train = X_train.values.reshape([-1, image_width, image_height, 1]).astype(\"float32\")\nX_valid = X_valid.values.reshape([-1, image_width, image_height, 1]).astype(\"float32\")\nX_test = X_test.values.reshape([-1, image_width, image_height, 1]).astype(\"float32\")\nprint('{} \\n{}'.format(X_train.shape, X_valid.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b5464abff9fb929cc8e2ec1f7ef0cf71fa6ec16"},"cell_type":"code","source":"def genAugmentedImg(img, label, num_data=10000):\n    gen_data = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    fill_mode='nearest')\n    \n    gen_data.fit(X_train)\n    data = gen_data.flow(img, label, batch_size=num_data)\n    \n    return data[0][0], data[0][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d6262572296733ca049d60173cd98d6cc4f0f79"},"cell_type":"code","source":"X_aug_train, y_aug_train = genAugmentedImg(X_train, y_train, 200) # used 20K for model\nX_aug_valid, y_aug_valid = genAugmentedImg(X_valid, y_valid, 50) # used 5K\n\nprint(\"X_aug_train shape :\", X_aug_train.shape)\nprint(\"y_aug_train shape :\", y_aug_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4ae2f024cc09554a52900460d78f08904f42044"},"cell_type":"code","source":"# JUST TAKING A LOOK - CALL ME CURIOUS\nfor i in range(0, 9):\n    image = X_aug_train[i, :, :, 0]\n    plt.subplot(3, 3, i+1)\n    plt.imshow(image, cmap=cm.binary)\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c9dd9501a51ce9e55714ffedf2030c620000917"},"cell_type":"code","source":"# YOU DON'T WANT THE AUGMENTED IMAGES USED FOR VALIDATION, SO JUST\n# LUMP THEM BACK IN WITH THE TRAINING SET\nX_train = np.concatenate((X_train, X_aug_train, X_aug_valid), axis=0)\ny_train = np.concatenate((y_train, y_aug_train, y_aug_valid), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ca8e3bf7501540feaeb9333402f47006c6604a2"},"cell_type":"code","source":"# TOTALLY UNNECESSARY, COULD JUST DO X_train = X_train/255.0\ndef normalizer(dframe): \n    dframe = (dframe - X_train.min()) / (X_train.max() - X_train.min())\n    return(dframe)\n\nX_train = normalizer(X_train)\nX_valid = normalizer(X_valid)\nX_test = normalizer(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2996ac5ac1c5d26e7db62c1e2d8dc646d850ffd1"},"cell_type":"code","source":"from keras.utils.np_utils import to_categorical \n\ny_train = to_categorical(y_train, num_classes = 10)\ny_valid= to_categorical(y_valid, num_classes = 10)\n\nnum_labels = y_train.shape[1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a2fea5c58204cdd1a9d34862071aaa521f17a6d"},"cell_type":"code","source":"# RAN A MINI TEST TO MAKE SURE THIS WORKS\ndef shuffleData(x, y): \n    idx = np.arange(0, x.shape[0])\n    np.random.shuffle(idx)    \n    return x[idx], y[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c6c15dcc4ee2b02ee4bb3a0f068454749655fb9"},"cell_type":"code","source":"# NOT BAD FOR A NEXT_BATCH FUNCTION. THIS DOES NOT ACCOUNT FOR REMAINDERS \n# AT THE TAIL END OF AN EPOCH. BUT THE SHUFFLE FUNCTION NEGATES \n# THE NEED FOR IT. REMAINDER OR NOT, THE IMAGES ARE RESHUFFLED \n# BETWEEN EPOCHS. SO EVERY IMAGE WILL BE USED.\nepochs_completed = 0\nindex_in_epoch = 0\n\ndef next_batch(x, y, batch_size):\n\n    global index_in_epoch\n    global epochs_completed\n       \n    num_examples = x.shape[0]\n    start = index_in_epoch\n    index_in_epoch += batch_size\n\n    if index_in_epoch > num_examples:\n        # finished epoch\n        epochs_completed += 1\n        # shuffle the data\n        x, y = shuffleData(x, y)\n        # start next epoch\n        start = 0\n        index_in_epoch = batch_size  \n    end = index_in_epoch\n    # we don't need to store past batches in memory \n    yield x[start:end], y[start:end]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97b1e44680a7f295fa98b222997ccf8df8b72994"},"cell_type":"code","source":"tf.reset_default_graph()\ntf.set_random_seed(81)\nnp.random.seed(81)\n\nalpha = 0.1\nx_kernel = tf.contrib.layers.xavier_initializer()\n\n\n# PLACEHOLDERS\nx = tf.placeholder(tf.float32, [None, 28, 28, 1])\ny = tf.placeholder(tf.float32, [None, num_labels])\nlearning_rate = tf.placeholder(tf.float32)\nnon_dropout = tf.placeholder(tf.float32)\n\n# INPUT\nimages = tf.reshape(x, [-1, image_width, image_height, num_colors])\n# (batch, height, width, channel)\n\n# CNN LAYERS\nconv1_a = tf.layers.conv2d(inputs=images, filters=32, kernel_size=5, strides=(1,1),\n                        padding='same',activation=tf.nn.relu,\n                        kernel_initializer=x_kernel)  \nconv1_b = tf.layers.conv2d(inputs=conv1_a, filters=32, kernel_size=5, strides=(1,1),\n                        padding='same',activation=tf.nn.relu,\n                        kernel_initializer=x_kernel)  \npool1 = tf.layers.max_pooling2d(conv1_b,pool_size=2, strides=2)\ndrop1 = tf.nn.dropout(pool1, keep_prob=0.75)\n\nconv2_a = tf.layers.conv2d(drop1, 64, 5, padding='same', activation=tf.nn.relu, \n                           kernel_initializer=x_kernel)\nconv2_b = tf.layers.conv2d(conv2_a, 64, 5, padding='same', activation=tf.nn.relu, \n                           kernel_initializer=x_kernel)\npool2 = tf.layers.max_pooling2d(conv2_b,pool_size=2, strides=2)\ndrop2 = tf.nn.dropout(pool2, keep_prob=0.75)\n\n\n# FULL CONNECTED LAYERS\nfc1_a = tf.contrib.layers.flatten(drop2)\nfc1_b = tf.layers.dense(fc1_a, 256, kernel_initializer=x_kernel)\nfc1_c = tf.layers.batch_normalization(fc1_b)\nfc1_d = tf.maximum(fc1_c, fc1_c*alpha)\nfc1_e = tf.nn.dropout(fc1_d, non_dropout)\n\nfc2_a = tf.contrib.layers.flatten(fc1_e)\nfc2_b = tf.layers.dense(fc2_a, 128, kernel_initializer=x_kernel)\nfc2_c = tf.layers.batch_normalization(fc2_b)\nfc2_d = tf.maximum(fc2_c, fc2_c*alpha)\nfc2_e = tf.nn.dropout(fc2_d, non_dropout)\n\nlogits = tf.layers.dense(fc2_e, 10, kernel_initializer=x_kernel)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\ncorrect_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\npred = tf.argmax(logits, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"394ea95b4d63f54aaa6561eb7d6094a36e715985"},"cell_type":"code","source":"# chart\nchart = {}\nchart[\"train_acc\"] = []\nchart[\"val_acc\"] = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56c48752309a1677a93b31789260592d44003f40"},"cell_type":"code","source":"def train(epochs, batch_size, keep_probability, lr):\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        for i in range(epochs + 1):\n\n            for x_batch, y_batch in next_batch(X_train, y_train, batch_size):  \n                sess.run(optimizer, feed_dict={x: x_batch, y: y_batch, non_dropout: keep_probability, learning_rate:lr})\n\n            accuracy_train = accuracy.eval({x: x_batch, y: y_batch, non_dropout: 1.0})\n            accuracy_val = accuracy.eval({x: X_valid, y: y_valid, non_dropout: 1.0})\n\n            # PRINT OUT A MESSAGE EVERY 100 STEPS\n            if i%100 == 0:\n                print('epoch {}'.format(i))\n                print('Training acc: {:.4f} \\n Validation acc: {:.4f}'.format(accuracy_train, accuracy_val))\n                \n                chart['train_acc'].append(accuracy_train)\n                chart['val_acc'].append(accuracy_val)\n                \n        output = pred.eval({x: X_test, non_dropout: 1.0})\n            \n    return output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc20134bce008a29e97c671468b3c76dc0567ee8"},"cell_type":"code","source":"# %%time\n# WILL RUN A SHORTENED VERSION WITH CPU \nepochs = 500 # This was 10000 with abovemention GPU, about 40min \nbatch_size = 128\nkeep_probability = 0.50\nlr = 0.001\n\npredictions = train(epochs, batch_size, keep_probability, lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55acb779da4a3506f85837caf056a925ee7c20d8"},"cell_type":"code","source":"df_out = pd.DataFrame(predictions)\ndf_out.index = [x + 1 for x in df_out.index]\ndf_out.rename(columns={0: 'Label'}, inplace=True)\ndf_out.index.name = \"ImageId\"\n\ndf_out.to_csv(\"predictions_4.csv\", index=True)\ndf_out.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"480fe5ed728f195aede56ed6c554b644a3e4e03a"},"cell_type":"code","source":"# TAKE A LOOK AT THE FIRST FEW. IF YOU GOT THE FIRST 5 CORRECT, \n# YOU'RE LOOKING GOOD. IF YOU MISSED THE 2ND ZERO ONLY, YOU'RE\n# STILL DOING OK. ANY MORE THAN THAT, BOO\nfor index in range(5):\n    plt.figure()\n    image = X_test[index,:]\n    plt.imshow(image.reshape(image_width, image_height), cmap=cm.binary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8121c3d81da770258fc4545b698a52200d09fc2"},"cell_type":"code","source":"# NOT REALLY HELPFUL WITH THE REDUCED DATASET\n# plot acc\nplt.plot(chart[\"train_acc\"])\nplt.plot(chart[\"val_acc\"])\nplt.title('Accuracy')\nplt.xlabel('epoch')\nplt.ylabel('acc')\nplt.legend(['train', 'validation'], loc=4),\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c19608859f696910d9adf89cea5f84782cb7c398"},"cell_type":"markdown","source":"> # On Datalab"},{"metadata":{"_uuid":"ea10df4f515cdcaedbf291ab7854b272555cbbd3"},"cell_type":"markdown","source":"You'll want to export your csv. The above code is still needed. All you need to run is the following:"},{"metadata":{"trusted":true,"_uuid":"add543d96b79524e1c034245eb2e8d21ea10ddef","_kg_hide-output":true},"cell_type":"code","source":"!gsutil cp 'predictions.csv' 'gs://path/predictions.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"493b9a9e17a78d17af3ed6af7ef66385faf5b5cc","_kg_hide-output":true},"cell_type":"code","source":"# NOW RUN THE FOLLOWING TO CONFIRM YOUR CSV HAS BEEN ADDED TO \n# YOUR BUCKET\n%%gcs list --object gs://pathtobucket","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"740352c3171582cd782c7616a84ef465f13c87e0"},"cell_type":"markdown","source":"If it has, navigate to your google cloud storage bucket on the website. Right-click on your csv and select Save Link As... Now you can submit your predictions to Kaggle!\n\n__Some other recommendations/tips:__ You'll need to create a datalab instance using command line (on either the cloud terminal or your own terminal) and launch the datalab jupyter notebook that way. You can create instances on the cloud website, but you may not be able to access datalab with it.  The following link should help: https://cloud.google.com/datalab/docs/quickstart. Watch out for fees. You'll need to shut down the instance when finished. Run the following command line:\n\ndatalab connect datalab-instance-name  \ndatalab list --filter 'status=RUNNING'  \ndatalab stop datalab-instance-name  \n\nI also empty all buckets when I'm finished. I racked up some expenses leaving a few large datasets in my buckets. \n\nSome of the above code came from the following kagglers:\n\nhttps://www.kaggle.com/sjun1008/mnist-with-pure-tensorflow-0-99542\nhttps://www.kaggle.com/villoro/cnn-with-tensorflow-dlfn-udacity\nand probably a few others that I'm forgetting. \n\nLet me know if you have any questions! I'm happy to help.\n\n\n\n\n\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}