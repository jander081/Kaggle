{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target\n",
    "---------------------\n",
    "\n",
    "__Overview:__ 2 sets of data in this 'kernels only' competition: News and Prices/Returns. Use both sets to predict the movement of a given financial asset in the next 10 days. _I believe it is a 10 day residual expected return. So this is one numeric prediction for each asset in the universe (??)_\n",
    "\n",
    "We have data from 2007 to 2017 for training and must predict the movement of assets from Jan 2017 to July 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will detail the entire project so far\n",
    "\n",
    "We've actually done quite a bit of work, but it is so disjointed that it is nearly impossible to progress\n",
    "in any meaningful way\n",
    "\n",
    "Need to Organize\n",
    "\n",
    "__SEE RESEARCH at bottom for links and additional notes__\n",
    "\n",
    "### TOC\n",
    "---------------\n",
    "\n",
    "__Scripts folder:__ \n",
    "\n",
    "1. Sigma_imports.py  -> Not updated\n",
    "2. AssetExpand.py -> left an old func; not yet tested in kernel\n",
    "3. sigma_1.py -> func for checking mem usage and submitting\n",
    "4. sigma_plot -> \n",
    "\n",
    "__Notebooks folder:__\n",
    "\n",
    "1. Output folder -> random output: text files, csvs\n",
    "\n",
    "2. Lightgbm_basic -> Attempts to use demos to run a small scale model. This obviously fails since the dates do not allow for a JOIN. Invested good amount of time trying to produce a small sample set that can be explored locally rather than on the kernel. The sample set exists, but is inaccurate. It should only be used in preprocessing.\n",
    "\n",
    "3. feature_analysis -> so far only type and value counts. Could perform EDA and correlation (filter methods)\n",
    "\n",
    "4. processing_speed -> Everything big data. Keep that kernel from crashing\n",
    "\n",
    "5. News_working (link with scratch_news) -> statsmodel (need to move), expandinging features into indicators, aggregation of expansion\n",
    "\n",
    "6. scratch_news -> Indicators, freqs, word vecs, DFS\n",
    "\n",
    "7. scratch_market -> a small sample is produced for experimentation. Additionally, market datasets pulled from datareader.\n",
    "\n",
    "\n",
    "__References folder:__\n",
    "\n",
    "1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notes:__ next day. Continue to develop sample and organize/abstract. \n",
    "\n",
    "Start to divide work between market and news. You can take large intervals of market data from data_reader and conduct analysis. In fact, you could entirely focus on the market data for a few days - it will benefit you in the interview process.   \n",
    "\n",
    "Think that's the best idea. Incorporate everything you've been working on, possibly BM as well. Pull the data and conduct backtests. The main holdup is the target variable. Consider addressing that first.  \n",
    "\n",
    "1. Research the market residual. \n",
    "2. Identify relevant market data.\n",
    "3. Focus exclusively on market data and begin submitting predictions\n",
    "\n",
    "__UNIVERSE__  \n",
    "Jury is still out. You should probably train on all. When you set your metric to track progress, the metric will use the universe feature for your score.\n",
    "\n",
    "\n",
    "__Target: returnsOpenNextMktres10__   \n",
    "market-residualized return. This is the TARGET VARIABLE used in competition scoring. The market data has been filtered such that returnsOpenNextMktres10 is always not null.  \n",
    "\n",
    "market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../scripts'\n",
    "import sys\n",
    "sys.path.append(path)\n",
    "from sigma_imports import *\n",
    "from sigma_code import *\n",
    "\n",
    "data_path = '/Users/jacob/Desktop/docs/kaggle/_2_sigma_news/_g_data/data'\n",
    "df_market = pd.read_csv(data_path + '/marketdata_sample.csv') \n",
    "df_news = pd.read_csv(data_path + '/news_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target review\n",
    "\n",
    "You are trying to predict a confidene value $y_t \\in [-1, 1]$ This value is a strength measure of the models prediction that the 10 day rez will be positive or negative. Therefore, this is _not_ a regression. But rather a classifier using probabilities of binary outcomes. The best way to understand this is by example:  \n",
    "\n",
    "For t = 3 and asset returns $[r_1, r_2, r_3, r_4]$ say my model predicts ```[-1, 1, 1, -1]``` for the target. The model, however, is only predicting $[0,1]$ s.t. any probability $\\ge .5 = 1$ and = 0 o.w.   \n",
    "1: positive return and 0: negative return.  \n",
    "\n",
    "So lets say the strength of those prediction (confidence predictions: $p_{3i}$) are as follows ```[0.3, 0.7, 0.8, 0.1]```. Therefore, the model is more/less confident on certain predictions. Now, say the target values are ```[-.005, .002, .03, .06]```, notice that the model missed a pred that it was fairly confident about. It was 90% confident that the return was going to be negative. Not only was it positive, but it was positive by a lot. This should hurt the score a lot more therefore. Let's also say that one of the assets (4) is not in the daily universe. \n",
    "\n",
    "For each day in the evaluation time period, we calculate:\n",
    "$$x_{t} = \\sum_{i}\\hat y_{ti}r_{ti}u_{ti}$$  \n",
    "\n",
    "where $r_{ti}$ is the 10-day market-adjusted leading return for day $t$ for instrument $i$, and $u_{ti}$ is a 0/1 `universe` variable (see the data description for details) that controls whether a particular asset is included in scoring on a particular day.\n",
    "\n",
    "For $\\hat{y}$, we need to perform the transformation of the confidence prediction: $\\hat{y}_{ti} = (p_{ti} * 2) - 1$. This allows for the following.  \n",
    "If the prediction is:  \n",
    "True positive (TP): $\\hat{y}_{ti} \\in \\mathbb{R}^+$ and $\\hat{y}_{ti}r_{ti} \\in \\mathbb{R}^+$  Since $p_{ti} > .5$  \n",
    "True Negative (TN): $\\hat{y}_{ti} \\in \\mathbb{R}^-$ and $\\hat{y}_{ti}r_{ti} \\in \\mathbb{R}^+$  Since $p_{ti} <.5$  \n",
    "\n",
    "Both results improve your score by a weight determined by the confidence and actual return.  \n",
    "FP and FN then take on weighted negative values\n",
    "\n",
    "So our $\\hat{y}_{3i}$ = ```[-0.4, 0.4, 0.6, -0.6]``` \n",
    "\n",
    "Cool. Let's plug in some values:\n",
    "\n",
    "$x_{3} = \\sum_{i=1}^4\\hat y_{3i}r_{3i}u_{3i} = (-.04 * -.005 * 1) +   (0.4 * .002 * 1) + (0.6 * 0.03 * 0) + (-0.6 * 0.06 * 1)\\\\ \n",
    "\\qquad = (0.0002 * 1) + (0.0008 * 1) + (0) + (-0.036 * 1)\\\\ \n",
    "\\qquad = -0.035\n",
    "$  \n",
    "Ouch! That missed prediction and universe condition really hurt.\n",
    "\n",
    "Then all of the daily scores are averaged \n",
    "\n",
    "Your submission score is then calculated as the mean divided by the standard deviation of your daily $x_t$ values:\n",
    "\n",
    "$$score = \\frac{\\bar x_{t}}{\\sigma(x_{t})}.$$\n",
    "\n",
    "Of course, higher scores are desireble, so less variation between daily scores is important. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRYING TO PREVENT DATE TYPE CONVERSION\n",
    "# THIS SHOULD BE UPDATED FROM PANDA DATAREADER\n",
    "data = pd.read_csv(data_path + '/sigma_data.csv', index_col='Date', parse_dates=True, \n",
    "                 infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>returns_close_raw</th>\n",
       "      <th>returns_open_raw</th>\n",
       "      <th>returns_close_raw10</th>\n",
       "      <th>returns_open_raw10</th>\n",
       "      <th>returns_open_raw10_next</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-19</th>\n",
       "      <td>CHL</td>\n",
       "      <td>52.790001</td>\n",
       "      <td>52.570000</td>\n",
       "      <td>1180200.0</td>\n",
       "      <td>0.037009</td>\n",
       "      <td>0.039607</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>-0.036273</td>\n",
       "      <td>-0.034444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-19</th>\n",
       "      <td>WMT</td>\n",
       "      <td>62.250000</td>\n",
       "      <td>62.560001</td>\n",
       "      <td>13051300.0</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.011309</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>0.028515</td>\n",
       "      <td>-0.078002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-19</th>\n",
       "      <td>NGG</td>\n",
       "      <td>73.569870</td>\n",
       "      <td>73.755455</td>\n",
       "      <td>692700.0</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>-0.020221</td>\n",
       "      <td>-0.020998</td>\n",
       "      <td>-0.054715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-19</th>\n",
       "      <td>AUO</td>\n",
       "      <td>2.660000</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>796100.0</td>\n",
       "      <td>0.011429</td>\n",
       "      <td>0.011342</td>\n",
       "      <td>-0.055263</td>\n",
       "      <td>-0.065478</td>\n",
       "      <td>0.046162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-19</th>\n",
       "      <td>SIEB</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>-0.059089</td>\n",
       "      <td>-0.033336</td>\n",
       "      <td>-0.114880</td>\n",
       "      <td>-0.089128</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           asset       Open      Close      Volume  returns_close_raw  \\\n",
       "Date                                                                    \n",
       "2016-01-19   CHL  52.790001  52.570000   1180200.0           0.037009   \n",
       "2016-01-19   WMT  62.250000  62.560001  13051300.0           0.010121   \n",
       "2016-01-19   NGG  73.569870  73.755455    692700.0           0.009518   \n",
       "2016-01-19   AUO   2.660000   2.640000    796100.0           0.011429   \n",
       "2016-01-19  SIEB   1.180000   1.150000      2300.0          -0.059089   \n",
       "\n",
       "            returns_open_raw  returns_close_raw10  returns_open_raw10  \\\n",
       "Date                                                                    \n",
       "2016-01-19          0.039607            -0.046641           -0.036273   \n",
       "2016-01-19          0.011309             0.017740            0.028515   \n",
       "2016-01-19          0.000297            -0.020221           -0.020998   \n",
       "2016-01-19          0.011342            -0.055263           -0.065478   \n",
       "2016-01-19         -0.033336            -0.114880           -0.089128   \n",
       "\n",
       "            returns_open_raw10_next  \n",
       "Date                                 \n",
       "2016-01-19                -0.034444  \n",
       "2016-01-19                -0.078002  \n",
       "2016-01-19                -0.054715  \n",
       "2016-01-19                 0.046162  \n",
       "2016-01-19                 0.017094  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual\n",
    "\n",
    "https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n",
    "\n",
    "We'll consider seasonal to be market. Frequency is set to the number of assets. This should end up giving daily residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "\n",
    "x = data[['returns_open_raw10_next']]\n",
    "# Freq will depend on asset number - this needs to be researched\n",
    "result = seasonal_decompose(x, model='additive', freq=50)\n",
    "\n",
    "data['returns_res10_next'] = result.resid\n",
    "# Lose the first and last day\n",
    "data = data[-data['returns_res10_next'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8022, 7) (8022, 1)\n",
      "(2396, 7) (2396, 1)\n",
      "(1125, 7) (1125, 1)\n",
      "[1]\tvalid_0's binary_logloss: 0.693885\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.694804\n",
      "[3]\tvalid_0's binary_logloss: 0.695783\n",
      "[4]\tvalid_0's binary_logloss: 0.695499\n",
      "[5]\tvalid_0's binary_logloss: 0.695503\n",
      "[6]\tvalid_0's binary_logloss: 0.696995\n",
      "[7]\tvalid_0's binary_logloss: 0.700037\n",
      "[8]\tvalid_0's binary_logloss: 0.700629\n",
      "[9]\tvalid_0's binary_logloss: 0.70292\n",
      "[10]\tvalid_0's binary_logloss: 0.703661\n",
      "[11]\tvalid_0's binary_logloss: 0.70439\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.693885\n"
     ]
    }
   ],
   "source": [
    "df['date'] = df.index.strftime(\"%Y%m%d\").astype(int)\n",
    "df['universe'] = universe_feat(df)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Needed for dataframe splits\n",
    "dates = df['date'].unique()\n",
    "\n",
    "# Convert targets to binary\n",
    "target = ['returns_res10_next']\n",
    "label = ['label']\n",
    "# df[target[0]] = (df[target[0]] > 0).astype(int)\n",
    "df[label[0]] = (df[target[0]] > 0).astype(int)\n",
    "# Consider vectorize - later\n",
    "\n",
    "def train_feats(dframe, drops=list):\n",
    "    # could do list comp\n",
    "    train_feat = []\n",
    "    for col in dframe.columns:\n",
    "        if col not in drops:\n",
    "            train_feat.append(col)\n",
    "    return train_feat\n",
    "\n",
    "feats = train_feats(df, drops=['date', 'asset', 'universe', \n",
    "                               'returns_open_raw10_next', 'returns_res10_next',\n",
    "                               'label'])\n",
    "\n",
    "train_range = range(len(dates))[:int(0.70*len(dates))]\n",
    "val_range = range(len(dates))[int(0.70*len(dates)):int(0.9*len(dates))]\n",
    "test_range = range(len(dates))[int(0.9*len(dates)):]\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# KINDA LOOKS CRAPPY. THIS JUST SETS THE TRAINING/VAL SETS WITH PREDICTORS\n",
    "# train data - numpys\n",
    "X_train = df[feats].fillna(0).loc[df['date'].isin(dates[train_range])].values\n",
    "Y_train = df[label].fillna(0).loc[df['date'].isin(dates[train_range])].values\n",
    "\n",
    "# GETS INTERESTING -> INPUTS DATA INTO INSTANCE\n",
    "lgb_train = lgb.Dataset(X_train, Y_train[:,0])\n",
    "print(X_train.shape, Y_train.shape)\n",
    "\n",
    "# validation data - numpys\n",
    "X_v = df[feats].fillna(0).loc[df['date'].isin(dates[val_range])].values\n",
    "Y_v = df[label].fillna(0).loc[df['date'].isin(dates[val_range])].values\n",
    "\n",
    "lgb_val = lgb.Dataset(X_v, Y_v[:,0]),\n",
    "print(X_v.shape, Y_v.shape)\n",
    "\n",
    "# test data\n",
    "X_test = df[feats].fillna(0).loc[df['date'].isin(dates[test_range])].values\n",
    "Y_test = df[label].fillna(0).loc[df['date'].isin(dates[test_range])].values\n",
    "\n",
    "print(X_test.shape, Y_test.shape)\n",
    "\n",
    "param = {\"objective\" : \"binary\",\n",
    "          \"metric\" : \"binary_logloss\",\n",
    "          \"verbosity\" : -1 }\n",
    "\n",
    "# TONS OF HYPERPARAMENTERS. THIS FIT FUNC USES THE VAL SET TO TRAIN AND \n",
    "# SELECT THE BEST ITERATION\n",
    "\n",
    "model = lgb.train(param, lgb_train, valid_sets=lgb_val, \n",
    "                  early_stopping_rounds=10) \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(date, pred_proba, num_target, universe):\n",
    "    y = pred_proba*2 - 1\n",
    "    r = num_target#.clip(-1,1) # get rid of outliers - not sure about this\n",
    "    # I see the logic, but you'd need to remove the corresponding preds, unv\n",
    "    x = y * r * universe\n",
    "    result = pd.DataFrame({'day' : date, 'x' : x})\n",
    "    x_t = result.groupby('day').sum().values\n",
    "    return np.mean(x_t) / np.std(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6416903792259109"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FULL TEST SET DATASET VERSION\n",
    "preds = model.predict(X_test, num_iteration=model.best_iteration) \n",
    "date_vec = df['date'].loc[df['date'].isin(dates[test_range])]\n",
    "u = df['universe'].loc[df['date'].isin(dates[test_range])]\n",
    "actual = df[target].fillna(0).loc[df['date'].isin(dates[test_range])].values[:, 0]\n",
    "\n",
    "\n",
    "custom_metric(date_vec, preds, actual, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5400366074034371"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MICRO VERSION\n",
    "X = df[feats].fillna(0).loc[df['date'].isin([20161114, 20161115])].values\n",
    "preds = model.predict(X, num_iteration=model.best_iteration) \n",
    "actual = df[target].fillna(0).loc[df['date'].isin([20161114, 20161115])].values\n",
    "u = df['universe'].fillna(0).loc[df['date'].isin([20161114, 20161115])].values\n",
    "date_vec = df['date'].loc[df['date'].isin([20161114, 20161115])].values\n",
    "\n",
    "custom_metric(date_vec, preds, actual[:, 0], u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5400366074034371"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WALKTHROUGH\n",
    "y = preds*2 - 1\n",
    "r = actual[:,0]   \n",
    "x = y*r*u\n",
    "result = pd.DataFrame({'day' : date_vec, 'x' : x})\n",
    "x_t = result.groupby('day').sum().values\n",
    "np.mean(x_t) / np.std(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for day in test_range:\n",
    "# for day in range(214, 216):\n",
    "    print(dates[day])\n",
    "    \n",
    "    X = df[feats].fillna(0).loc[df['date'] == dates[day]].values\n",
    "    u = df['universe'].fillna(0).loc[df['date'] == dates[day]].values\n",
    "    # use target not label\n",
    "    r = df[target].fillna(0).loc[df['date'] == dates[day]].values\n",
    "    \n",
    "    y = model.predict(X, num_iteration=model.best_iteration)*2 - 1\n",
    "    # shape needs to be adjusted. This corrects for universe\n",
    "    r_u = r[:, 0]*u\n",
    "    \n",
    "    xt = r_u.T @ y\n",
    "    \n",
    "    print(xt)\n",
    "    total.append(xt)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHOULD BE 24 days\n",
    "print(x.shape)\n",
    "len(set(df.date))\n",
    "# test data\n",
    "X_test_df = (\n",
    "            df.\n",
    "            fillna(0).\n",
    "            loc[df['date'].\n",
    "            isin(dates[test_range])]\n",
    ")\n",
    "len(set(X_test_df.date))\n",
    "# Confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8724795888022177"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(total) / np.std(total)\n",
    "# Pretty good? I don't trust it then. Of course, this is over a small test \n",
    "# interval. So the model is probably getting lucky. Plus, this is not resisualized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Look to implement this  \n",
    "```from sklearn.model_selection import TimeSeriesSplit```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Links to check out:\n",
    "\n",
    "https://www.kaggle.com/nareyko/fast-lags-calculation-concept-using-numpy-arrays\n",
    "\n",
    "Fundamental data?  \n",
    "\n",
    "https://pypi.org/project/wallstreet/0.1.5/\n",
    "\n",
    "https://alphascientist.com/feature_selection.html\n",
    "\n",
    "Read this first - sound very close to a market residualized\n",
    "https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
